{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Samuel Larkin's Knowledge Base Documentation","text":"<p>As I find new tools or new tricks, I document those in this repository.</p>"},{"location":"color/","title":"Colors","text":""},{"location":"color/#bash-colors","title":"Bash Colors","text":"<p>Script to display all terminal colors</p> <pre><code>msgcat --color=test\n</code></pre>"},{"location":"color/#putty-colors","title":"Putty Colors","text":"<p>We need to use <code>putty-256color</code> instead of <code>xterm-256color</code> or else the <code>Home</code> key is not working.</p> <p>Putty shows prompt with no color, but Linux SSH can In Putty, change Settings -&gt; Connection &gt; Data &gt; Terminal-type string to: <code>putty-256color</code>. \"Emulate\" 256 colors in PuTTY terminal</p> <ol> <li> <p>Configure Putty    In Settings &gt; Windows &gt; Colours there is a check box for \"Allow terminal to use xterm 256-colour mode\".</p> </li> <li> <p>Let the app know    You'll probably have to change Settings -&gt; Connection &gt; Data &gt; Terminal-type string to: <code>xterm-256color</code>    if your server has a terminfo entry for <code>putty-256color</code>, typically in <code>/usr/share/terminfo/p/putty-256color</code>, you can set <code>Putty</code>'s Terminal-Type to <code>putty-256color</code> instead.    The main thing here is to make the server use an available <code>Terminfo</code> entry that most closely matches the way <code>Putty</code> is configured.</p> </li> </ol>"},{"location":"color/#tmux","title":"<code>tmux</code>","text":"<p><code>tmux</code> will display darker color, making it almost impossible to read unless you change to <code>putty-256color</code>. If you already have a running <code>tmux</code> server, you can dynamically fix it by doing:</p> <pre><code>tmux set-environment -g TERM \"xterm\"\n</code></pre>"},{"location":"color/#24bit","title":"24bit","text":"<ul> <li>Getting 24-bit color working in terminals</li> </ul>"},{"location":"color/#lscolors-schemes","title":"LSCOLORS Schemes","text":"<pre><code>for theme in $(vivid themes); do\n  echo \"Theme: $theme\";\n  LS_COLORS=$(vivid generate $theme);\n  ls;\n  echo;\ndone\nvivid generate one-light\nLSCOLORS=$(vivid generate one-light)\n</code></pre>"},{"location":"config_files/","title":"Configuration Files","text":"<ul> <li>Difference Between <code>.bashrc</code>, <code>.bash-profile</code>, and <code>.profile</code></li> <li>What are the functional differences between .profile .bash_profile and .bashrc</li> </ul> <p><code>.bash_profile</code> and <code>.bashrc</code> are specific to <code>bash</code>, whereas <code>.profile</code> is read by many shells in the absence of their own shell-specific config files. (<code>.profile</code> was used by the original Bourne shell.) <code>.bash_profile</code> or <code>.profile</code> is read by login shells, along with <code>.bashrc</code>; subshells read only <code>.bashrc</code>. (Between job control and modern windowing systems, <code>.bashrc</code> by itself doesn't get used much. If you use <code>screen</code> or <code>tmux</code>, screens/windows usually run subshells instead of login shells.)</p> <p>The idea behind this was that one-time setup was done by <code>.profile</code> (or shell-specific version thereof), and per-shell stuff by <code>.bashrc</code>. For example, you generally only want to load environment variables once per session instead of getting them whacked any time you launch a subshell within a session, whereas you always want your aliases (which aren't propagated automatically like environment variables are).</p> <p>Other notable shell config files:</p> <p><code>/etc/bash_profile</code> (fallback <code>/etc/profile</code>) is read before the user's .profile for system-wide configuration, and likewise <code>/etc/bashrc</code> in subshells (no fallback for this one). Many systems including Ubuntu also use an <code>/etc/profile.d</code> directory containing shell scriptlets, which are <code>.</code> (<code>source</code>)-ed from <code>/etc/profile</code>; the fragments here are per-shell, with <code>*.sh</code> applying to all Bourne/POSIX compatible shells and other extensions applying to that particular shell.</p>"},{"location":"fzf/","title":"fzf","text":""},{"location":"fzf/#tmux-fzf","title":"<code>tmux-fzf</code>","text":"<p>tmux-fzf: Use fzf to manage your tmux work environment!</p> <ul> <li>List of bindings</li> <li>prefix F To launch tmux-fzf, press <code>prefix</code> + <code>F</code> (Shift+F).</li> </ul>"},{"location":"fzf/#fzf-gitsh","title":"<code>fzf-git.sh</code>","text":"<p>fzf-git.sh: bash and zsh key bindings for Git objects, powered by fzf.</p> <ul> <li>List of bindings</li> <li>CTRL-G CTRL-F for Files</li> <li>CTRL-G CTRL-B for Branches</li> <li>CTRL-G CTRL-T for Tags</li> <li>CTRL-G CTRL-R for Remotes</li> <li>CTRL-G CTRL-H for commit Hashes</li> <li>CTRL-G CTRL-S for Stashes</li> <li>CTRL-G CTRL-E for Each ref (<code>git for-each-ref</code>)     &gt; :warning: You may have issues with these bindings in the following cases:     &gt;     &gt; - CTRL-G CTRL-B will not work if     &gt;   CTRL-B is used as the tmux prefix     &gt; - CTRL-G CTRL-S will not work if flow control is enabled,     &gt;   CTRL-S will freeze the terminal instead     &gt;   - (<code>stty -ixon</code> will disable it)     &gt;     &gt; To workaround the problems, you can use     &gt; CTRL-G {key} instead of     &gt; CTRL-G CTRL-{KEY}.</li> <li>Inside fzf</li> <li>TAB or SHIFT-TAB to select multiple objects</li> <li>CTRL-/ to change preview window layout</li> <li>CTRL-O to open the object in the web browser (in GitHub URL scheme)</li> </ul>"},{"location":"git/","title":"Git","text":""},{"location":"git/#pat-and-github","title":"PAT and github","text":"<p>Alternative: Use <code>url</code> insteadOf with Personal Access Tokens (PAT) If you prefer a lightweight, token-based approach (especially for automation), configure Git to inject your PAT directly into URLs using <code>insteadOf</code>.</p> <p>Set up a PAT-based URL replacement:</p> <pre><code>git config --global url.\"https://api:${GITHUB_TOKEN}@github.com/\".insteadOf \"https://github.com/\"\n</code></pre> <p>Replace <code>${GITHUB_TOKEN}</code> with your actual GitHub Personal Access Token (required if 2FA is enabled). This works for SSH and HTTPS URLs:</p> <pre><code>git config --global url.\"https://api:${GITHUB_TOKEN}@github.com/\".insteadOf \"ssh://git@github.com/\"\ngit config --global url.\"https://api:${GITHUB_TOKEN}@github.com/\".insteadOf \"git@github.com:\"\n</code></pre> <p>\u26a0\ufe0f Note: This method embeds the token in Git config, so avoid using it on shared or public machines.</p>"},{"location":"git/#git-cli","title":"Git CLI","text":""},{"location":"git/#remove-remote-branch-from-local","title":"Remove Remote Branch from Local","text":"<p>How to remove a remote branch ref from local (gh-pages)</p> <pre><code>git update-ref -d refs/remotes/origin/BRANCH\n</code></pre>"},{"location":"git/#how-to-delete-a-remote-branch","title":"How to Delete a Remote Branch","text":"<pre><code>git push --delete origin BRANCH\n</code></pre> <p>or</p> <pre><code>git push origin :BRANCH\n</code></pre>"},{"location":"git/#realign-a-branch-with-origin","title":"Realign a Branch with <code>origin</code>","text":"<p>When you want to make your branch <code>BRANCH</code> the same as <code>origin/BRANCH</code> no matter what.</p> <pre><code>git swith BRANCH\ngit reset --hard origin/BRANCH\n</code></pre>"},{"location":"git/#temporarily-disabled-delta-to-get-a-patch","title":"Temporarily Disabled <code>delta</code> to Get a Patch","text":"<p>To get a patch out of <code>git diff</code>, by the command to <code>less</code> which changes the pager from <code>delta</code> to <code>less</code>.</p> <pre><code>git diff | less\n</code></pre>"},{"location":"git/#github","title":"GitHub","text":""},{"location":"git/#failed-ci-activity","title":"Failed CI activity","text":"<p>Delete all failed <code>CI activity</code> for a given user. Delete GitHub workflow runs using the gh cli</p> <pre><code>gh run list --status failure --user samuellarkin --json databaseId -q '.[].databaseId' \\\n| parallel --jobs  1 \"gh api repos/$(gh repo view --json nameWithOwner -q .nameWithOwner)/actions/runs/{} -X DELETE\"\n</code></pre>"},{"location":"git/#push-an-approved-prs-branch","title":"Push an Approved PR's branch","text":"<p>Once a PR is approved, you can use the following command to merge your <code>dev/work</code> branch to <code>main</code> given that your branch is at the tip of <code>main</code>. This effectively does a fast forward push from the CLI.</p> <pre><code>git push origin origin/dev/work:main\n</code></pre>"},{"location":"git/#find-which-pr-a-commit-belongs-to","title":"Find Which PR a Commit Belongs to","text":"<p>When you want to know a commit what added during which Pull Request:</p> <pre><code>git clone --mirror git@github.com:EveryVoiceTTS/EveryVoice.git EveryVoice-mirror\ncd EveryVoice-mirror/\nsed -i 's|refs/pull/|refs/heads/pull/|' packed-refs\ngit log --all --graph --decorate --oneline   # or your favourite compact log\n</code></pre>"},{"location":"huggingface/","title":"HuggingFace","text":""},{"location":"huggingface/#preprocess_logits_for_metrics","title":"preprocess_logits_for_metrics","text":"<p>preprocesslogits_for_metrics (<code>Callable[[torch.Tensor, torch.Tensor], torch.Tensor]</code>, _optional): W [1/7] A function that preprocess the logits right before caching them at each evaluation step. Must take two tensors, the logits and the labels, and return the logits once processed as desired. The modifications made by this function will be reflected in the predictions received by <code>compute_metrics</code>.</p> <p>Note that the labels (second parameter) will be <code>None</code> if the dataset does not have them.</p>"},{"location":"install/","title":"Install","text":""},{"location":"install/#uv","title":"uv","text":"<pre><code>uv venv --relocatable --python-preference=only-managed  --python=3.12 --prompt=my_env venv\n</code></pre>"},{"location":"install/#unslothai","title":"Unsloth.ai","text":""},{"location":"install/#on-gpsc7","title":"On GPSC7","text":"<ul> <li>Set proxies(http and https) and correct <code>tmp</code> directories in <code>.profile</code></li> <li><code>nrc_profile.sh -c</code> : To update the profile</li> <li>Created user directory and set symlinks for <code>.cache</code> and <code>.conda</code></li> <li><code>conda create --name unsloth_env python=3.10</code></li> <li><code>source activate unsloth_env</code> : I didn't load moniconda module because that was introducing python 3.9 in the system path and I was not able to remove it.</li> <li><code>conda install pytorch-cuda=12.1 pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers</code></li> <li><code>pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"</code></li> <li><code>pip install --no-deps trl peft accelerate bitsandbytes</code></li> <li>I still got torch not found error, even though Step 7 was successful, so I just reinstalled it and then it worked.</li> <li>Launch jupyter in a job with <code>partition=gpu_a100</code>, <code>account=nrc_ict__gpu_a100</code> and <code>qos=low</code></li> </ul>"},{"location":"json/","title":"JSON/JSONL","text":"<p>jq manual</p>"},{"location":"json/#scriptjq","title":"script.jq","text":"<p>To make a <code>jq</code> script executable:</p> <pre><code>#!/usr/bin/env -S jq --monochrome-output --compact-output --from-file\n# -S, --split-string=S\n\n# your `jq` filters\n</code></pre>"},{"location":"json/#cookbook","title":"Cookbook","text":""},{"location":"json/#aggregate-a-field","title":"Aggregate a Field","text":"<p>Given a list of objects where some of them have the same <code>id</code> but with a field with different values, aggregate that field for each object. This happens when you extracted data from <code>mysql</code>. <code>mysql</code> doesn't allow subqueries to return multiple rows with multiple columns thus you have to do the same work using <code>JOIN</code>. Merge Arrays of JSON</p> <pre><code>echo -e '{\"id\":1, \"b\":[{\"c\":1}]}{\"id\":1, \"b\":[{\"c\":2}]}'\n</code></pre> <pre><code>{\n  \"id\": 1,\n  \"b\": [\n    {\n      \"c\": 1\n    }\n  ]\n}\n{\n  \"id\": 1,\n  \"b\": [\n    {\n      \"c\": 2\n    }\n  ]\n}\n</code></pre> <ul> <li>group entries by <code>id</code></li> <li>for each group</li> <li>take the first element and aggregate all of the <code>b</code> in a list</li> <li>return that first element that has been augmented with a list of <code>b</code></li> </ul> <pre><code>echo -e '{\"id\":1, \"b\":[{\"c\":1}]}{\"id\":1, \"b\":[{\"c\":2}]}' \\\n| jq --slurp 'group_by(.id) | .[] | (.[0].b=([.[].b]|flatten)) | .[0]'\n</code></pre> <pre><code>{\n  \"id\": 1,\n  \"b\": [\n    {\n      \"c\": 1\n    },\n    {\n      \"c\": 2\n    }\n  ]\n}\n</code></pre>"},{"location":"json/#compare","title":"Compare","text":"<p>Compare two jsonl files that are not in the same order. This implies that we need to sort the files on some <code>key</code>. Do a trick a la <code>Schwartian transform</code> where we prepend the sort <code>key</code>, sort on that <code>key</code> and the remove the <code>key</code>.</p> <pre><code>jqdiff \\\n  &lt;(zcat train.jsonl.gz \\\n    | jq --compact-output --raw-output '[.id, .|@text] | @tsv' \\\n    | sort -k1,1 \\\n    | cut -f2,2) \\\n  &lt;(zcat source/train.jsonl.gz \\\n    | jq --compact-output --raw-output '[.id, .|@text] | @tsv' \\\n    | sort -k1,1 \\\n    | cut -f 2,2)\n</code></pre> <p>Otherwise, use the alias <code>jqdiff</code> which essentially does</p> <pre><code>vimdiff &lt;(jq --sort-keys . file1.json) &lt;(jq --sort-keys . file2.json)\n</code></pre>"},{"location":"json/#convert-to-array","title":"Convert to Array","text":"<p>Given</p> <pre><code>[{ \"seg\": [\"A\", \"B\"] }, { \"seg\": \"C\" }]\n</code></pre> <p>The second object is NOT an array but you need it to be an array to process all elements the same way, you can make sure all segments are arrays by doing:</p> <pre><code>jq '.[] | .seg | (if type == \"object\" then [.] else . end) | .[]'\n</code></pre>"},{"location":"json/#counting-elements","title":"Counting Elements","text":"<p>Count the number of entries/sentence pairs that have the <code>.unparsable</code> key.</p> <pre><code>pv Huge.jsonl \\\n| jq --null-input '[ inputs | select(.unparsable)] | reduce .[] as $item (0; . + 1)'\n</code></pre>"},{"location":"json/#filename","title":"Filename","text":"<p>If you need the file name in your script, you can call the function <code>input_filename</code>.</p> <pre><code>jq '[input_filename, input_line_number]' translation/nmt/test.en2fr.scores.json\n</code></pre> <pre><code>[\n  \"translation/nmt/test.en2fr.scores.json\",\n  53\n]\n</code></pre>"},{"location":"json/#filter-out-subobjects","title":"Filter-out SubObjects","text":"<p>Given</p> <pre><code>&lt;?xml version='1.0' encoding='utf-8'?&gt;\n&lt;dataset id=\"wmttest2024\"&gt;\n  &lt;collection id=\"general\"&gt;\n    &lt;doc origlang=\"en\" id=\"test-en-news_beverly_press.3585\" domain=\"news\"&gt;\n      &lt;src lang=\"en\"&gt;\n        &lt;p&gt;\n          &lt;seg id=\"1\"&gt;Siso's depictions of land, water center new gallery exhibition&lt;/seg&gt;\n        &lt;/p&gt;\n      &lt;/src&gt;\n      &lt;ref lang=\"es\" translator=\"refA\"&gt;\n        &lt;p&gt;\n          &lt;seg id=\"1\"&gt;Representaciones de la tierra y el agua de Siso centran una nueva exposici\u00f3n&lt;/seg&gt;\n        &lt;/p&gt;\n      &lt;/ref&gt;\n    &lt;/doc&gt;\n    &lt;doc origlang=\"en\" id=\"test-en-news_brisbanetimes.com.au.228963\" domain=\"NOT_news\"&gt;\n      &lt;src lang=\"en\"&gt;\n        &lt;p&gt;\n          &lt;seg id=\"1\"&gt;Adapt the old, accommodate the new to solve issue&lt;/seg&gt;\n        &lt;/p&gt;\n      &lt;/src&gt;\n      &lt;ref lang=\"es\" translator=\"refA\"&gt;\n        &lt;p&gt;\n          &lt;seg id=\"1\"&gt;Adapta lo viejo, incorpora lo nuevo para resolver el problema&lt;/seg&gt;\n        &lt;/p&gt;\n      &lt;/ref&gt;\n    &lt;/doc&gt;\n  &lt;/collection&gt;\n&lt;/dataset&gt;\n</code></pre> <p>Remove documents that are NOT of <code>news</code> domain keeping the document's structure.</p> <pre><code>~/.local/bin/yq 'del(.dataset.collection.doc[] | select(.[\"+@domain\"] != \"news\"))' wmttest2024.en-es.xml\n</code></pre> <pre><code>&lt;?xml version='1.0' encoding='utf-8'?&gt;\n&lt;dataset id=\"wmttest2024\"&gt;\n  &lt;collection id=\"general\"&gt;\n    &lt;doc origlang=\"en\" id=\"test-en-news_beverly_press.3585\" domain=\"news\"&gt;\n      &lt;src lang=\"en\"&gt;\n        &lt;p&gt;\n          &lt;seg id=\"1\"&gt;Siso's depictions of land, water center new gallery exhibition&lt;/seg&gt;\n        &lt;/p&gt;\n      &lt;/src&gt;\n      &lt;ref lang=\"es\" translator=\"refA\"&gt;\n        &lt;p&gt;\n          &lt;seg id=\"1\"&gt;Representaciones de la tierra y el agua de Siso centran una nueva exposici\u00f3n&lt;/seg&gt;\n        &lt;/p&gt;\n      &lt;/ref&gt;\n    &lt;/doc&gt;\n  &lt;/collection&gt;\n&lt;/dataset&gt;\n</code></pre>"},{"location":"json/#flat-files-to-structured-json","title":"Flat Files to Structured json","text":"<p>When you have multiple flat files that you want to combine into a structured json.</p> <p>lingua_eng_spa/Tilde-worldbank-1-eng-spa.spa.gz</p> <pre><code>SPA     0.9998978843092705\nSPA     0.9991979235059277\n</code></pre> <p>lingua_all_languages/Tilde-worldbank-1-eng-spa.spa.gz</p> <pre><code>SPA     0.9999975457963204\nSPA     0.9847735076254288\n</code></pre> <p>Tilde-worldbank-1-eng-spa.spa.gz</p> <pre><code>\"Igualmente, hacemos notar la importancia de abordar el problema del hambre y la malnutrici\u00f3n\u201d.\n\"La vida es muy dif\u00edcil.\n</code></pre> <p>Tilde-worldbank-1-eng-spa.eng.gz</p> <pre><code>\" We also note the importance of addressing hunger and malnutrition.\u201d\n\"[Life] is extremely difficult.\n</code></pre> <pre><code>paste \\\n  &lt;(zcat lingua_eng_spa/Tilde-worldbank-1-eng-spa.spa.gz) \\\n  &lt;(zcat lingua_all_languages/Tilde-worldbank-1-eng-spa.spa.gz) \\\n  &lt;(zcat Tilde-worldbank-1-eng-spa.spa.gz) \\\n  &lt;(zcat Tilde-worldbank-1-eng-spa.eng.gz) \\\n| mlr --tsv --ojson --implicit-csv-header \\\n  label eng_spa.lid,eng_spa.confidence,all.lid,all.confidence,spa,eng \\\n| jq '.[]'\n</code></pre> <pre><code>{\n  \"eng_spa\": {\n    \"lid\": \"SPA\",\n    \"confidence\": 0.9998978843092705\n  },\n  \"all\": {\n    \"lid\": \"SPA\",\n    \"confidence\": 0.9999975457963204\n  },\n  \"spa\": \"\\\"Igualmente, hacemos notar la importancia de abordar el problema del hambre y la malnutrici\u00f3n\u201d.\",\n  \"eng\": \"\\\" We also note the importance of addressing hunger and malnutrition.\u201d\"\n}\n{\n  \"eng_spa\": {\n    \"lid\": \"SPA\",\n    \"confidence\": 0.9991979235059277\n  },\n  \"all\": {\n    \"lid\": \"SPA\",\n    \"confidence\": 0.9847735076254288\n  },\n  \"spa\": \"\\\"La vida es muy dif\u00edcil.\",\n  \"eng\": \"\\\"[Life] is extremely difficult.\"\n}\n</code></pre>"},{"location":"json/#group-by-x-and-merge","title":"Group by X and Merge","text":"<p>Context: after generating <code>*.scores.json</code> using <code>sacrebleu  --width=14 reference --metrics bleu chrf ter  &lt; translation &gt; scores.json</code>. Can we extract BLEU scores from all our experiments and tabulate the result using <code>mlr</code>?</p> <pre><code>find -type f -name \\*scores.json \\\n| xargs dirname \\\n| parallel 'jq \"{\\\"expt_name\\\": \\\"{//}\\\",  \\\"{/}\\\": (.[] | select(.name == \\\"BLEU\\\") | .score)}\" {}/*scores.json' \\\n| jq --slurp --sort-keys 'group_by(.expt_name) | [.[] | add]' \\\n| mlr --json --opprint --barred cat \\\n| less\n</code></pre>"},{"location":"json/#parallel-processing","title":"Parallel Processing","text":"<p>Note that we use <code>--keep-order</code>, <code>--spreadstdin</code> &amp; <code>--recend='\\n'</code>.</p> <pre><code>function process {\n   cat\n}\nexport -f process\n\nzcat input.gz \\\n    | time \\parallel \\\n       --keep-order \\\n       --spreadstdin \\\n       --recend='\\n' \\\n       --env=process \\\n       'process' \\\n| gzip \\\n&gt; output.gz\n</code></pre>"},{"location":"json/#value-in-a-set","title":"Value in a Set","text":"<p>Select entries that have a field value within a set.</p> <pre><code>jq 'select(.BlockID == (1742974, 1742975))' data/hoc-log-20241218-blocks.json\n</code></pre>"},{"location":"json/#xml-to-json","title":"XML to json","text":"<p>Using yq, we can convert a xml document into a json file.</p> <pre><code>yq -p xml -o json &lt; input.xml &gt; output.json\n</code></pre>"},{"location":"json/#zip-multiple-files","title":"Zip Multiple files","text":"<p>Merge arrays The key here is the <code>transpose</code>.</p> <pre><code>zcat translation.fr.json.gz \\\n| jq \\\n    --slurp \\\n    --argfile src &lt;(jq --raw-input '{\"src\":.}' source.en) \\\n    --argfile ref &lt;(jq --raw-input '{\"ref\":.}' reference.fr) \\\n    '[., $src, $ref] | transpose | map(add) | .[]'\n</code></pre>"},{"location":"miller/","title":"miller","text":"<ul> <li>Documentation</li> <li>GitHub: Miller is like awk, sed, cut, join, and sort for name-indexed data such as CSV, TSV, and tabular JSON.</li> </ul>"},{"location":"miller/#scriptmlr","title":"script.mlr","text":"<p>Scripting with Miller In the following example, we simply convert a <code>jsonl</code> file to <code>tsv</code>. To make a <code>miller</code> script executable:</p> <pre><code>#!/usr/bin/env -S mlr -s\n# -S, --split-string=S\n# -s {file name} Take command-line flags from file name. For more\n#                information please see\n#                https://miller.readthedocs.io/en/latest/scripting/.\n\n\n--ijsonl --otsv\ncat\n</code></pre>"},{"location":"miller/#cookbook","title":"Cookbook","text":""},{"location":"miller/#find-hoc-sittings-elapsed-time","title":"Find HoC Sittings Elapsed Time","text":"<pre><code>bzcat sentence_word_count_fr.tsv.bz2 | head -n 3\n</code></pre> <pre><code>id      datetime        wc      sentence\nHouse/House/391/Debates/001/HAN001      2006-04-03 11:05:00.000000      49      The 38th Parliament ...\nHouse/House/391/Debates/001/HAN001      2006-04-03 11:05:00.001000      5       Monday, April 3, 2006\n</code></pre> <ul> <li>Transform <code>$datestamp</code> into the number of second since epoch 0</li> <li>Find the minimum and maximum datetime for each Sitting</li> <li>Figure out the elpase time</li> <li>Convert the start and end time back to a datetime</li> <li>Discard unwanted fields by specifying which one we want to keep</li> <li>Reorder the fields</li> </ul> <pre><code>bzcat sentence_word_count_fr.tsv.bz2 \\\n| head -n 10000 \\\n| mlr --itsv --opprint --barred \\\n  cut -x -f sentence then put '$datestamp = strptime($datetime, \"%Y-%m-%d %T.%f\")' \\\n  then \\\n  stats1 -g id -f datestamp -a min,max \\\n  then \\\n  put '$elapse = $datestamp_max - $datestamp_min'\n  then \\\n  put '$start = strftime($datestamp_min, \"%Y-%m-%d %T\"); $end = strftime($datestamp_max, \"%Y-%m-%d %T\")' \\\n  then \\\n  cut -f id,start,end,elapse \\\n  then \\\n  reorder -f id,start,end,elapse\n</code></pre> <pre><code>+------------------------------------+---------------------+---------------------+--------------------+\n| id                                 | start               | end                 | elapse             |\n+------------------------------------+---------------------+---------------------+--------------------+\n| House/House/391/Debates/001/HAN001 | 2006-04-03 11:05:00 | 2006-04-03 14:05:00 | 10800.12400007248  |\n| House/House/391/Debates/002/HAN002 | 2006-04-04 00:00:00 | 2006-04-04 17:05:00 | 61500.206000089645 |\n| House/House/391/Debates/003/HAN003 | 2006-04-05 00:00:00 | 2006-04-05 18:25:00 | 66300.73900008202  |\n| House/House/391/Debates/004/HAN004 | 2006-04-06 00:00:00 | 2006-04-06 23:15:01 | 83701.97799992561  |\n| House/House/391/Debates/005/HAN005 | 2006-04-07 00:00:00 | 2006-04-07 14:30:00 | 52200.70899987221  |\n| House/House/391/Debates/006/HAN006 | 2006-04-10 00:00:00 | 2006-04-10 11:35:00 | 41700.10199999809  |\n+------------------------------------+---------------------+---------------------+--------------------+\n</code></pre>"},{"location":"miller/#group-per-date","title":"Group per Date","text":"<pre><code>jq --raw-output --compact-output \\\n   '{\"date\": (.timestamp|split(\" \")[0]), \"fr\": (.fr|split(\" \") | length), \"en\": (.en|split(\" \")|length)}' \\\n| mlr --ijson \\\n  then stats1 -a sum,count -f en,fr -g date \\\n  then cut -x -f en_count \\\n  then rename en_sum,#en_word,fr_sum,#fr_word,fr_count,#sentence \\\n| mlr --opprint --barred summary -a count,null_count,distinct_count,mean,min,median,max,stddev\n</code></pre> <pre><code>{\"date\":\"2022-10-24\",\"fr\":18,\"en\":15}\n{\"date\":\"2022-10-24\",\"fr\":18,\"en\":18}\n{\"date\":\"2022-10-24\",\"fr\":29,\"en\":28}\n</code></pre> <pre><code>date=2022-10-24,#en_word=55951,#fr_word=59892,#sentence=2692\ndate=2022-10-25,#en_word=73587,#fr_word=79288,#sentence=3660\ndate=2022-10-26,#en_word=29726,#fr_word=32800,#sentence=1492\n</code></pre> field_name count null_count distinct_count mean stddev min median max date 112 0 112 2022-10-24 2023-03-23 2023-09-29 #en_word 112 0 112 64893.669642857145 25599.55527604576 2812 68386 123193 #fr_word 112 0 112 71302.02678571429 27863.87682146542 3150 76163 133257 #sentence 112 0 112 3282.6160714285716 1318.6068832416186 128 3502 6373"},{"location":"miller/#group-per-object","title":"Group per Object","text":"<pre><code>jq --raw-output --compact-output \\\n   '{\"sitting\": (.doc | {parliament, session, number})} + {\"fr\": (.fr|split(\" \") | length), \"en\": (.en|split(\" \")|length)}' \\\n| mlr --ijson \\\n   then stats1 -a sum,count -f en,fr -g sitting \\\n   then cut -x -f en_count \\\n   then rename en_sum,#en_word,fr_sum,#fr_word,fr_count,#sentence \\\n| mlr --opprint --barred summary -a count,null_count,distinct_count,mean,min,median,max,stddev\n</code></pre> <pre><code>{\"sitting\":{\"parliament\":44,\"session\":1,\"number\":116},\"fr\":18,\"en\":15}\n{\"sitting\":{\"parliament\":44,\"session\":1,\"number\":116},\"fr\":18,\"en\":18}\n{\"sitting\":{\"parliament\":44,\"session\":1,\"number\":116},\"fr\":29,\"en\":28}\n</code></pre> <pre><code>sitting.parliament=44,sitting.session=1,sitting.number=116,#en_word=55951,#fr_word=59892,#sentence=2692\nsitting.parliament=44,sitting.session=1,sitting.number=117,#en_word=73587,#fr_word=79288,#sentence=3660\nsitting.parliament=44,sitting.session=1,sitting.number=118,#en_word=29726,#fr_word=32800,#sentence=1492\n</code></pre> field_name count null_count distinct_count mean stddev min median max sitting.parliament 111 0 1 44 0 44 44 44 sitting.session 111 0 1 1 0 1 1 1 sitting.number 111 0 111 171.44144144144144 32.61697402069964 116 171 227 #en_word 111 0 111 65478.2972972973 25569.833917971166 23026 68386 131905 #fr_word 111 0 110 71944.38738738738 27816.245672487912 25521 76163 142068 #sentence 111 0 110 3312.189189189189 1315.06589889158 1147 3502 6588"},{"location":"miller/#tabulate-bleu-scores","title":"Tabulate BLEU Scores","text":"<ul> <li>Reading a csv dataframe</li> <li>Write a nice table using bars</li> <li>Print numbers with 2 decimal</li> <li>cut: keep column based on a list of regular expressions</li> <li>merge-fields: add a <code>mean</code> column calculating the mean of columns</li> <li>reorder: move the datetime column at the end of the table</li> <li>label: renamed the columns</li> <li>sort: on <code>test</code> to rank the rows</li> <li>put: add the row's rank</li> <li>sort: reorder <code>on expt_name</code></li> </ul> <pre><code>  score-tool tabulate --no-title \\\n  | mlr --icsv --opprint --barred --ofmt %.2f \\\n    cut -rf expt_name,$suffix,date \\\n    then merge-fields  -a mean  -r $suffix  -o $suffix  -k \\\n    then reorder -e -f datetime \\\n    then label expt_name,test,validation,mean,datetime \\\n    then sort -nr test \\\n    then put 'begin {@rank = 1} $rank = @rank; @rank += 1' \\\n    then sort -nr expt_name\n</code></pre>"},{"location":"miscellaneous/","title":"Tips-and-Tricks","text":""},{"location":"miscellaneous/#activate-custom-scripts","title":"Activate Custom Scripts","text":"<p>When running experiments, we often have a <code>venv</code> and a couple of custom scripts and aliases. This templates is used to enable all those.</p> <pre><code># :vim:filetype=bash:\n\ntool_dir=$(readlink -m $(dirname \"${BASH_SOURCE[0]}\"))\necho $tool_dir\n\ntest -f $tool_dir/../venv/bin/activate &amp;&amp; source $tool_dir/../venv/bin/activate \"\"\n[ -f $tool_dir/aliases ] &amp;&amp; source $tool_dir/aliases \"\"\nexport PATH=$tool_dir${PATH:+:$PATH}\n\nunset tool_dir\n</code></pre>"},{"location":"miscellaneous/#activate-template-for-common-software","title":"Activate Template for Common Software","text":"<p>This is an example of an <code>activate</code> script when you compile a tool by hand and you don't install it a common place.</p> <pre><code>############ SENTENCEPIECE ############\n# Set this variable to override where SentencePiece is installed\n\nexport SENTENCEPIECE_HOME=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" &gt;/dev/null 2&gt;&amp;1 &amp;&amp; pwd )\"\n\n# Home\nexport SENTENCEPIECE_HOME=${SENTENCEPIECE_HOME_OVERRIDE:-$SENTENCEPIECE_HOME}\n\n# Binaries\n[[ \"$PATH:\" == *\"$SENTENCEPIECE_HOME/bin\"* ]] || export PATH=$SENTENCEPIECE_HOME/bin:$PATH\n\n# Libraries\n[[ \"$PATH:\" == *\"$SENTENCEPIECE_HOME/bin\"* ]] || export LIBRARY_PATH=$SENTENCEPIECE_HOME/lib64${LIBRARY_PATH:+:$LIBRARY_PATH}\n[[ \"$PATH:\" == *\"$SENTENCEPIECE_HOME/lib64\"* ]] || export LD_LIBRARY_PATH=$SENTENCEPIECE_HOME/lib64${LD_LIBRARY_PATH:+:$LD_LIBRARY_PATH}\n[[ \"$PATH:\" == *\"$SENTENCEPIECE_HOME/lib64\"* ]] || export LD_RUN_PATH=$SENTENCEPIECE_HOME/lib64${LD_RUN_PATH:+:$LD_RUN_PATH}\n\n# Includes\n[[ \"$PATH:\" == *\"$SENTENCEPIECE_HOME/include\"* ]] || export CPLUS_INCLUDE_PATH=$SENTENCEPIECE_HOME/include${CPLUS_INCLUDE_PATH:+:$CPLUS_INCLUDE_PATH}\n\n# Package Configuration for ./configure to work when building other packages.\n[[ \"$PATH:\" == *\"$SENTENCEPIECE_HOME/lib/pkconfig\"* ]] || export PKG_CONFIG_PATH=$SENTENCEPIECE_HOME/lib/pkgconfig${PKG_CONFIG_PATH:+:$PKG_CONFIG_PATH}\n</code></pre>"},{"location":"miscellaneous/#bash-debugging","title":"BASH Debugging","text":"<ul> <li>Bash debugging - Youtube</li> <li><code>PS4</code> <code>export PS4='${BASH_SOURCE}:${LINENO}: ${FUNCNAME[0]}() - [${SHLVL},${BASH_SUBSHELL},$?] '</code></li> <li><code>bash -x</code></li> <li><code>bashdb</code></li> <li><code>shellcheck</code></li> </ul>"},{"location":"miscellaneous/#broken-symlinks","title":"Broken Symlinks","text":"<pre><code>find . -type l ! -exec test -e {} \\; -print\n</code></pre> <p>Here's an example to fix symlinks that need to have a level added to them.</p> <pre><code>function fix_link {\n  local -r link=${1:?LINK?}\n  ln -fns ../$(readlink $link) $link\n}\n</code></pre>"},{"location":"miscellaneous/#disk-usage","title":"Disk Usage","text":""},{"location":"miscellaneous/#tools","title":"Tools","text":"<ul> <li>diskus: A minimal, fast alternative to 'du -sh'</li> <li>dua: View disk space usage and delete unwanted data, fast.</li> <li>duc: Duc is a collection of tools for inspecting and visualizing disk usage</li> <li>dust: A more intuitive version of du in rust</li> <li>dutree: a tool to analyze file system usage written in Rust</li> <li>gdu: Fast disk usage analyzer with console interface written in Go</li> <li>godu: Simple golang utility helping to discover large files/folders.</li> <li>pdu: Highly parallelized, blazing fast directory tree analyzer</li> <li>tin-summer: Find build artifacts that are taking up disk space</li> </ul>"},{"location":"miscellaneous/#view-disk-usage-by-filetype","title":"View disk usage by filetype","text":"<pre><code>dust -t\n</code></pre> <pre><code> 3.0K   \u250c\u2500\u2500 (others)      \u2502                                             \u2588 \u2502   0%\n 2.0K   \u251c\u2500\u2500 .BLEU         \u2502                                             \u2588 \u2502   0%\n 2.0K   \u251c\u2500\u2500 .CHRF         \u2502                                             \u2588 \u2502   0%\n 2.0K   \u251c\u2500\u2500 .TER          \u2502                                             \u2588 \u2502   0%\n  64K   \u251c\u2500\u2500 .en           \u2502                                             \u2588 \u2502   0%\n  64K   \u251c\u2500\u2500 .sh           \u2502                                             \u2588 \u2502   0%\n  64K   \u251c\u2500\u2500 .train        \u2502                                             \u2588 \u2502   0%\n  64K   \u251c\u2500\u2500 .xz           \u2502                                             \u2588 \u2502   0%\n 128K   \u251c\u2500\u2500 .sharditer    \u2502                                             \u2588 \u2502   0%\n 130K   \u251c\u2500\u2500 .0            \u2502                                             \u2588 \u2502   0%\n 192K   \u251c\u2500\u2500 .log          \u2502                                             \u2588 \u2502   0%\n 832K   \u251c\u2500\u2500 .00000        \u2502                                             \u2588 \u2502   0%\n 1.3M   \u251c\u2500\u2500 (no extension)\u2502                                             \u2588 \u2502   0%\n 2.1M   \u251c\u2500\u2500 .fr           \u2502                                             \u2588 \u2502   0%\n 2.1M   \u251c\u2500\u2500 .gz           \u2502                                             \u2588 \u2502   0%\n 2.6M   \u251c\u2500\u2500 .out          \u2502                                             \u2588 \u2502   0%\n 3.3M   \u251c\u2500\u2500 .json         \u2502                                             \u2588 \u2502   0%\n 4.9M   \u251c\u2500\u2500 .word         \u2502                                             \u2588 \u2502   0%\n 799M   \u251c\u2500\u2500 .00001        \u2502                                    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502  20%\n 3.1G   \u251c\u2500\u2500 .pkl          \u2502         \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502  80%\n 3.9G \u250c\u2500\u2534 (total)         \u2502\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502 100%\n</code></pre>"},{"location":"miscellaneous/#filtering","title":"Filtering","text":""},{"location":"miscellaneous/#filtering-tsv-files-on-column","title":"Filtering tsv Files on Column","text":"<p>Filtering tsv files based on a subset of columns. Provided by Marc.</p> <pre><code># Filter-in\nawk \\\n  -F'\\t' \\\n  'NR==FNR{a[$4,$5];next} ($4,$5) in a' \\\n  uniq.DEVTEST_2022_${BIFILTER}.tsv \\\n  uniq.TRAIN_2021-2016_${BIFILTER}.tsv \\\n&gt; TRAIN_indev.tsv\n</code></pre> <pre><code># Filter-out\nawk \\\n  -F'\\t' \\\n  'NR==FNR{a[$4,$5];next} !(($4,$5) in a)' \\\n  uniq.DEVTEST_2022_${BIFILTER}.tsv \\\n  uniq.TRAIN_2021-2016_${BIFILTER}.tsv \\\n&gt; TRAIN_notindev.tsv\n</code></pre>"},{"location":"miscellaneous/#filter-out-testset-from-train","title":"Filter-out Testset From Train","text":"<pre><code>grep --text --line-regexp --invert-match --fixed-strings --file=$testset_filename\n</code></pre>"},{"location":"miscellaneous/#gnu-parallel-a-la-spark","title":"GNU parallel a la Spark","text":"<pre><code>function desubtokenize {}\nexport -f desubtokenize\n\nfunction tokenize_corpus {}\nexport -f tokenize_corpus\n\nzcat --force train.gz \\\n| parallel \\\n  --spreadstdin \\\n  --recend '\\n' \\\n  --env desubtokenize \\\n  --env tokenize_corpus \\\n  \"desubtokenize | tokenize_corpus $lang\" \\\n&gt; train.tok.gz\n</code></pre>"},{"location":"miscellaneous/#grep-for-emojis","title":"Grep for Emojis","text":"<p>POSIX and Unicode character categories</p> <pre><code>ugrep '\\p{So}' input_file\n</code></pre>"},{"location":"miscellaneous/#keep-sentence-pairs-for-sockeyes-length-limit","title":"Keep Sentence Pairs for Sockeye's Length Limit","text":"<p>Here's an example that keep sentence pairs that have less than 5 tokens.</p> <pre><code>paste &lt;(zcat OPUS-multiun-v1-eng-spa.spa.gz) &lt;(zcat OPUS-multiun-v1-eng-spa.eng.gz) \\\n| awk \\\n    -F'\\t' \\\n    'BEGIN {OFS = FS}  (split($1, a, \" +\")&lt;5 &amp;&amp; split($2, b, \" +\")&lt; 5) {print $1, $2}'\n</code></pre>"},{"location":"miscellaneous/#login-name","title":"Login Name","text":"<p>Find the full name of a user from its username.</p> <pre><code>lslogins | fzf\n</code></pre>"},{"location":"miscellaneous/#no-newline-at-end-of-file","title":"No newline at end of file","text":"<p>How to add a newline to the end of a file?</p> <pre><code>sed -i -e '$a\\' file\n</code></pre>"},{"location":"miscellaneous/#refresh-bashs-cache","title":"Refresh Bash's Cache","text":"<p>How do I clear Bash's cache of paths to executables? <code>bash</code> does cache the full path to a command. You can verify that the command you are trying to execute is hashed with the type command:</p> <pre><code>type svnsync\nsvnsync is hashed (/usr/local/bin/svnsync)\n</code></pre> <p>To clear the entire cache:</p> <p><code>hash -r</code></p> <p>Or just one entry:</p> <p><code>hash -d svnsync</code></p> <p>For additional information, consult help hash and man bash.</p>"},{"location":"miscellaneous/#seeded-shuf","title":"Seeded <code>shuf</code>","text":"<pre><code>function get_seeded_random {\n  local -r seed=\"$1\"\n  openssl enc -aes-256-ctr -pass pass:\"$seed\" -nosalt &lt;/dev/zero 2&gt;/dev/null\n}\n\nshuf -i1-100 --random-source=&lt;(get_seeded_random 42)\n</code></pre>"},{"location":"miscellaneous/#sort","title":"Sort","text":"<p>Sort according to a set of columns:</p> <pre><code>zcat FILE.gz | awk -F'\\t' '!_[$4,$5]++'\n</code></pre>"},{"location":"miscellaneous/#time","title":"Time","text":"<p>To get an time output that is easier to parse.</p> <pre><code>command time --portability python my_script.py\n</code></pre> <pre><code>real 3588.63\nuser 3333.36\nsys 519.36\n</code></pre>"},{"location":"miscellaneous/#watch-filesdirectories-then-take-action","title":"Watch Files/Directories then Take Action","text":"<p>If you want to take action when a directory gets created, you can use <code>watchexec</code>. Note, there used to be a <code>--emit-events-to environment</code> but it is phasing-out.</p> <pre><code>watchexec \\\n  --emit-events-to file \\\n  --watch compare-mt.output/ \\\n  --fs-events=create \\\n  --filter='checkpoint-?00' \\\n  --filter='checkpoint-*000' \\\n  \"parallel --dry-run --plus 'sbatch translate.slurm {##create:} en2fr' :::: \\$WATCHEXEC_EVENTS_FILE\"\n</code></pre>"},{"location":"miscellaneous/#weather","title":"Weather","text":"<p>wttr.in - GitHub: The right way to check the weather Get the weather:</p> <ul> <li><code>curl wttr.in/CityName</code></li> <li><code>curl v2d.wttr.in/CityName</code></li> </ul>"},{"location":"miscellaneous/#where-is-the-process-running","title":"Where is the process running","text":"<p>If there is a running process like <code>vim</code> that you would like to properly stop, you need to find in which <code>tmux</code> window it is running. To help figure this out, given the PID, you can ask <code>lsof</code> for its <code>CWD</code>.</p> <pre><code>lsof -a -d cwd -p PID\n</code></pre> <pre><code>COMMAND  PID    USER   FD   TYPE DEVICE SIZE/OFF      NODE NAME\nvim     7688 larkins  cwd    DIR   0,47     4096 154447160 /gpfs/projects/DT/mtp/models/HoC-Senate/corpora/spm/v2\n</code></pre>"},{"location":"miscellaneous/#conda","title":"conda","text":"<p>Build environment spec from explicit specs in history.</p> <pre><code>conda env export --from-history --prefix \"$prefix\" &gt; \"$prefix.from-history.yaml\"\n</code></pre>"},{"location":"miscellaneous/#setfacl","title":"setfacl","text":"<pre><code>   setfacl -m g:blz_acl_mtp:rwx git\n   getfacl --access git | setfacl -d -M- git\n</code></pre> <p>Change the default for the common group. Apply only to directories.</p> <pre><code>find corpora/ models/ pkgs/ venv/ -type d \\\n| xargs setfacl -d -m g:ai4dcluster-dt-mtp-project:rwx\n</code></pre>"},{"location":"nvim/","title":"Neovim","text":"<ul> <li>Popular Neovim Configurations</li> <li>Christian Chiarulli's neovim config</li> </ul>"},{"location":"nvim/#tips-and-tricks","title":"Tips-And-Tricks","text":"<ul> <li>Is it possible to disable lsp formatting temporarily?   You can avoid all autocommands with <code>:noa w</code>, or ignore some (or all) for some time with <code>:set eventignore=BufWritePre</code>.</li> <li>Replace <code>gq{motion}</code> by <code>gw{motion}</code> to format the lines that {motion} moves over.</li> </ul>"},{"location":"nvim/#lazyvim","title":"LazyVim","text":"<p>LazyVim is a collection of plugins and is built on top of lazy.vim. LazyVim is a Neovim setup powered by \ud83d\udca4 lazy.nvim to make it easy to customize and extend your config.</p> <ul> <li>LazyVim</li> <li>GitHub: Neovim config for the lazy</li> <li>GitHub - lazy.vim: \ud83d\udca4 A modern plugin manager for Neovim</li> </ul>"},{"location":"nvim/#treesitter","title":"TreeSitter","text":"<p>TreeSitter in <code>nvim</code> is used for better syntax highlighting.</p> <p>Tree-sitter is a parser generator tool and an incremental parsing library. It can build a concrete syntax tree for a source file and efficiently update the syntax tree as the source file is edited. Tree-sitter aims to be:</p> <ul> <li>General enough to parse any programming language</li> <li>Fast enough to parse on every keystroke in a text editor</li> <li>Robust enough to provide useful results even in the presence of syntax errors</li> <li> <p>Dependency-free so that the runtime library (which is written in pure C) can be embedded in any application</p> </li> <li> <p>tree-sitter: An incremental parsing system for programming tools</p> </li> <li>Documentation</li> <li>nvim-treesitter: Nvim Treesitter configurations and abstraction layer</li> </ul>"},{"location":"nvim/#nvim-treesitter-textobjects","title":"nvim-treesitter-textobjects","text":"<p>github: Syntax aware text-objects, select, move, swap, and peek support.</p>"},{"location":"nvim/#nvim-treesitter-context","title":"nvim-treesitter-context","text":"<p>github: Show the function you are in at the top of the buffer.</p> <p>A Vim plugin that shows the context of the currently visible buffer contents. It's supposed to work on a wide range of file types, but is probably most useful when looking at source code files. In most programming languages this context will show you which function you're looking at, and within that function which loops or conditions are surrounding the visible code.</p>"},{"location":"nvim/#telescope","title":"Telescope","text":""},{"location":"nvim/#telescopenvim","title":"telescope.nvim","text":"<p>github: Find, Filter, Preview, Pick. All lua, all the time.</p>"},{"location":"nvim/#telescope-fzf-nativenvim","title":"telescope-fzf-native.nvim","text":"<p>github: FZF sorter for telescope written in c. <code>fzf-native</code> is a <code>c</code> port of <code>fzf</code>. It only covers the algorithm and implements few functions to support calculating the score.</p>"},{"location":"nvim/#which-keynvim","title":"which-key.nvim","text":"<p>github: \ud83d\udca5 Create key bindings that stick. WhichKey is a lua plugin for Neovim 0.5 that displays a popup with possible keybindings of the command you started typing.</p>"},{"location":"nvim/#gitsignsnvim","title":"gitsigns.nvim","text":"<p>Equivalent to VCSVimdiff</p> <p>github: Git integration for buffers. Super fast git decorations implemented purely in Lua.</p> <p>Features</p> <ul> <li>Signs for added, removed, and changed lines</li> <li>Asynchronous using luv</li> <li>Navigation between hunks</li> <li>Stage hunks (with undo)</li> <li>Preview diffs of hunks (with word diff)</li> <li>Customizable (signs, highlights, mappings, etc)</li> <li>Status bar integration</li> <li>Git blame a specific line using virtual text.</li> <li>Hunk text object</li> <li>Automatically follow files moved in the index.</li> <li>Live intra-line word diff</li> <li>Ability to display deleted/changed lines via virtual lines.</li> <li>Support for yadm</li> <li>Support for detached working trees.</li> </ul>"},{"location":"nvim/#vim-illuminate","title":"vim-illuminate","text":"<p>github: illuminate.vim - (Neo)Vim plugin for automatically highlighting other uses of the word under the cursor using either LSP, Tree-sitter, or regex matching.</p>"},{"location":"nvim/#troublenvim","title":"trouble.Nvim","text":"<p>github: \ud83d\udea6 A pretty diagnostics, references, telescope results, quickfix and location list to help you solve all the trouble your code is causing.</p>"},{"location":"nvim/#language-sever-protocol-lsp","title":"Language Sever Protocol (LSP)","text":""},{"location":"nvim/#masonnvim","title":"mason.nvim","text":"<p>github: Portable package manager for Neovim that runs everywhere Neovim runs. Easily install and manage LSP servers, DAP servers, linters, and formatters.</p> <p><code>:h mason-introduction</code></p> <p><code>mason.nvim</code> is a Neovim plugin that allows you to easily manage external editor tooling such as LSP servers, DAP servers, linters, and formatters through a single interface. It runs everywhere Neovim runs (across Linux, macOS, Windows, etc.), with only a small set of external requirements needed.</p> <p>Packages are installed in Neovim's data directory (<code>:h standard-path</code>) by default. Executables are linked to a single <code>bin/</code> directory, which <code>mason.nvim</code> will add to Neovim's PATH during setup, allowing seamless access from Neovim builtins (shell, terminal, etc.) as well as other 3rd party plugins.</p> <p>For a list of all available packages, see https://mason-registry.dev/registry/list.</p>"},{"location":"nvim/#noicenvim","title":"noice.nvim","text":"<p>github: \ud83d\udca5 Highly experimental plugin that completely replaces the UI for messages, cmdline and the popupmenu.</p>"},{"location":"nvim/#miniindentscopre","title":"mini.indentscopre","text":"<p>github: Neovim Lua plugin to visualize and operate on indent scope. Part of 'mini.nvim' library.</p>"},{"location":"nvim/#lualinenvim","title":"lualine.nvim","text":"<p>github: A blazing fast and easy to configure neovim statusline plugin written in pure lua.</p>"},{"location":"nvim/#luasnip","title":"LuaSnip","text":"<p>github: Snippet Engine for Neovim written in Lua.</p>"},{"location":"nvim/#nvim-cmp","title":"nvim-cmp","text":"<p>github: A completion plugin for neovim coded in Lua. A completion engine plugin for neovim written in Lua. Completion sources are installed from external repositories and \"sourced\".</p>"},{"location":"nvim/#cmp-nvim-lsp","title":"cmp-nvim-lsp","text":"<p>github: nvim-cmp source for neovim builtin LSP client.</p> <p>Language servers provide different completion results depending on the capabilities of the client. Neovim's default omnifunc has basic support for serving completion candidates. <code>nvim-cmp</code> supports more types of completion candidates, so users must override the capabilities sent to the server such that it can provide these candidates during a completion request. These capabilities are provided via the helper function <code>require('cmp_nvim_lsp').default_capabilities</code></p> <p>As these candidates are sent on each request, adding these capabilities will break the built-in omnifunc support for neovim's language server client. <code>nvim-cmp</code> provides manually triggered completion that can replace omnifunc. See <code>:help cmp-faq</code> for more details.</p>"},{"location":"nvim/#cmp-buffer","title":"cmp-buffer","text":"<p>github: nvim-cmp source for buffer words.</p>"},{"location":"nvim/#cmp-path","title":"cmp-path","text":"<p>github: nvim-cmp source for filesystem paths.</p>"},{"location":"nvim/#cmp_luasnip","title":"cmp_luasnip","text":"<p>github: luasnip completion source for nvim-cmp</p>"},{"location":"nvim/#minipairs","title":"mini.pairs","text":"<p>github: Neovim Lua plugin to automatically manage character pairs. Part of 'mini.nvim' library.</p>"},{"location":"nvim/#minisurround","title":"mini.surround","text":"<p>github: Neovim Lua plugin with fast and feature-rich surround actions. Part of 'mini.nvim' library.</p>"},{"location":"nvim/#miniai","title":"mini.ai","text":"<p>github: Neovim Lua plugin to extend and create <code>a</code>/<code>i</code> textobjects. Part of 'mini.nvim' library.</p>"},{"location":"nvim/#minicomment","title":"mini.comment","text":"<p>github: Neovim Lua plugin for fast and familiar per-line commenting. Part of 'mini.nvim' library.</p>"},{"location":"nvim/#nvim-notify","title":"nvim-notify","text":"<p>github: A fancy, configurable, notification manager for NeoVim.</p>"},{"location":"nvim/#vim","title":"vim","text":""},{"location":"nvim/#editing-remote-files-via-scp","title":"Editing remote files via scp","text":"<pre><code>vimdiff local_file scp://remoteuser@server.tld//absolute/path/to/document\n</code></pre>"},{"location":"ollama/","title":"oLLaMa","text":""},{"location":"ollama/#using-huggingfaces-models-with-ollama","title":"Using HuggingFace's Models with oLLaMa","text":"<p>Some models are already quantized on HuggingFace. For example CohereLabs/c4ai-command-r7b-12-2024 has 23 quantized models. Selecting bartowski/c4ai-command-r7b-12-2024-GGUF, we can see on the top right corner of that page a drop down menu <code>Use this model</code>. In that list, there is a <code>oLLaMa</code> entry that copies to the clipboard</p> <pre><code>ollama run hf.co/bartowski/c4ai-command-r7b-12-2024-GGUF:Q4_K_M\n</code></pre>"},{"location":"ollama/#converting-your-own-model-to-ollama","title":"Converting your Own Model to oLLaMa","text":"<p>I don't recall on which site I found this but it was suggested to first go to f16 then quantize for better results.</p> <ul> <li>Ollama Model File</li> </ul>"},{"location":"ollama/#first-in-f16","title":"First in f16","text":"<p>First convert to F16.</p>"},{"location":"ollama/#setup","title":"Setup","text":"<pre><code>git clone git@github.com:ggerganov/llama.cpp.git\ncd llama.cpp\nuv venv --relocatable --python=3.12 --prompt=llama.cpp venv\nsource venv/bin/activate\nuv pip install numpy torch sentencepiece pyaml safetensors\nuv pip install .\nexport HF_HOME=$models/HuggingFace\n</code></pre>"},{"location":"ollama/#run","title":"Run","text":"<pre><code>llama-convert-hf-to-gguf \\\n  --outtype f16 \\\n  --outfile models--Unbabel--TowerInstruct-13B-v0.1-{ftype}.gguf \\\n  $HF_HOME/hub/models--Unbabel--TowerInstruct-13B-v0.1/snapshots/3965e508b334b28422969bf9a87fddbe6ee95b7f/\n</code></pre> <pre><code>llama-convert-hf-to-gguf \\\n  --outtype f16 \\\n  --outfile ModelSpace--GemmaX2-28-9B-v0.1-{ftype}.gguf \\\n  $HF_HOME/hub/models--ModelSpace--GemmaX2-28-9B-v0.1/snapshots/bd1bc3359faeb3cd01abb04ce470b410c2cc95d0\n</code></pre>"},{"location":"ollama/#quantization","title":"Quantization","text":""},{"location":"ollama/#setup_1","title":"Setup","text":"<p>From <code>llama.cpp/</code> build <code>llama-quantize</code>.</p> <pre><code>mkdir build\ncd build\n$HOME/opt/cmake-3.30.0/bin/cmake ..\nmake\n</code></pre>"},{"location":"ollama/#run_1","title":"Run","text":"<pre><code>$HOME/git/llama.cpp/build/bin/llama-quantize \\\n  Unbabel--TowerInstruct-13B-v0.1-f16.gguf \\\n  Unbabel--TowerInstruct-13B-v0.1-f16.ggml-Q4_K_M.gguf \\\n  Q4_K_M\n</code></pre> <pre><code>$HOME/git/llama.cpp/build/bin/llama-quantize \\\n  ModelSpace--GemmaX2-28-9B-v0.1-f16.gguf \\\n  ModelSpace--GemmaX2-28-9B-v0.1-f16.ggml-Q4_K_M.gguf \\\n  Q4_K_M\n</code></pre>"},{"location":"ollama/#converting-to-ollama","title":"Converting to oLLaMa","text":"<pre><code>ollama create --file Unbabel--TowerInstruct-13B-v0.1.Modelfile Unbabel--TowerInstruct-13B-v0.1\nollama create --file Unbabel--TowerInstruct-13B-v0.1-Q4_K_M.Modelfile Unbabel--TowerInstruct-13B-v0.1-Q4_K_M\n</code></pre> <p>where <code>Unbabel--TowerInstruct-13B-v0.1-Q4_K_M.Modelfile</code></p> <pre><code>FROM $MODELS/transformers/hub/models--Unbabel--TowerInstruct-13B-v0.1/snapshots/3965e508b334b28422969bf9a87fddbe6ee95b7f/ggml-model-Q4_K_M.gguf\n</code></pre> <p>Can we make a model that automatically translate French to English by changing the template to incorporate the instruction? The following commands embed the template in the model.</p> <pre><code>ollama create --file Unbabel--TowerInstruct-13B-v0.1-Q4_K_M.en2fr.Modelfile Unbabel--TowerInstruct-13B-v0.1-Q4_K_M.en2fr\nollama create --file Unbabel--TowerInstruct-13B-v0.1-Q4_K_M.fr2en.Modelfile Unbabel--TowerInstruct-13B-v0.1-Q4_K_M.fr2en\nollama create --file ModelSpace--GemmaX2-28-9B-v0.1-Q4_K_M.en2fr.Modelfile  ModelSpace--GemmaX2-28-9B-v0.1-Q4_K_M.en2fr\nollama create --file ModelSpace--GemmaX2-28-9B-v0.1-Q4_K_M.fr2en.Modelfile  ModelSpace--GemmaX2-28-9B-v0.1-Q4_K_M.fr2en\n</code></pre> <p>Create models we no specific template.</p> <pre><code>ollama create --file Unbabel--TowerInstruct-13B-v0.1-Q4_K_M.Modelfile  Unbabel--TowerInstruct-13B-v0.1-Q4_K_M\nollama create --file ModelSpace--GemmaX2-28-9B-v0.1-Q4_K_M.Modelfile ModelSpace--GemmaX2-28-9B-v0.1-Q4_K_M\n</code></pre> <p>Where <code>ModelSpace--GemmaX2-28-9B-v0.1-Q4_K_M.fr2en.Modelfile</code></p> <pre><code>FROM ModelSpace--GemmaX2-28-9B-v0.1-f16.ggml-Q4_K_M.gguf\n\nTEMPLATE \"\"\"Translate this from French to English:\\nFrench: {{ .Prompt }}\\nEnglish:\"\"\"\n</code></pre> <p>Where <code>Unbabel--TowerInstruct-13B-v0.1-Q4_K_M.fr2en.Modelfile</code></p> <pre><code>FROM models--Unbabel--TowerInstruct-13B-v0.1-f16.ggml-Q4_K_M.gguf\n\nTEMPLATE \"\"\"{{- range .Messages }}&lt;|im_start|&gt;{{ .Role }}\nTranslate the following text from French into English.\\nFrench: {{ .Content }}\\nEnglish:&lt;|im_end|&gt;\n{{ end }}&lt;|im_start|&gt;assistant\n\"\"\"\n</code></pre>"},{"location":"ollama/#translate","title":"Translate","text":"<pre><code>ollama run Unbabel--TowerInstruct-13B-v0.1-Q4_K_.fr2enM &lt; corpora/C13-1202-1333.fr\nollama run Unbabel--TowerInstruct-13B-v0.1-Q4_K_.fr2enM &lt; corpora/C14-0508-1335.fr\n</code></pre>"},{"location":"perl/","title":"Perl","text":""},{"location":"perl/#print-unicode-codepoint","title":"Print Unicode Codepoint","text":"<p>Here, we have a list of character counts in a corpus file <code>train.characters</code>. We would like to see the character's unicode codepoint followed by its count. WARNING: You need to use <code>&lt; file</code> or else this script fails.</p> <pre><code>\u146e 346\n\u146f 850\n\u1470 110\n\u1472 888\n\u1473 638\n\u1475 36\n</code></pre> <pre><code>perl -CI -Mutf8 -ple 's/([^ ]+)/sprintf(\"U+%04X\", ord($1))/e' &lt; train.char_count\n</code></pre> <ul> <li><code>-CIO</code> enables UTF-8 on input (<code>-CI</code>) and output (<code>-CO</code>)</li> <li><code>-Mutf8</code> tells Perl the source is UTF-8</li> <li><code>split</code> // splits the line into characters</li> <li><code>ord($c)</code> gets the code point, <code>printf</code> formats it as <code>U+XXXX</code></li> </ul> <pre><code>U+146F 850\nU+1470 110\nU+1472 888\nU+1473 638\nU+1475 36\n</code></pre>"},{"location":"perl/#remove-repeating-chinese-characters","title":"Remove Repeating Chinese Characters","text":"<pre><code>perl -ple 'BEGIN{use utf8;use open qw(:std :utf8);} s/(\\p{Han})\\1{1,}/\\1/gm' &lt; translation.zho.word\n</code></pre>"},{"location":"putty/","title":"Putty","text":""},{"location":"putty/#making-a-tunnel","title":"Making a Tunnel","text":"<p>Right-click with the mouse in the window's title bar.</p> <p></p> <p>Then click <code>Change Settings... &gt; Connection &gt; SSH &gt; Tunnels</code>.</p> <p>Choose a <code>Source port</code> like <code>2024</code> and a <code>Destination</code> <code>localhost:2044</code>.</p> <p>CLICK <code>Add</code> THEN <code>Apply</code></p> <p>Note that the ports can be different but it is simpler to remember if you use the same port number in both the <code>Source port</code> and the <code>Destination</code>.</p> <p></p> <p>Your tunnel should be operational.  Now use a browser and type <code>localhost:2024</code> or change the port to whatever you chose.</p> <p>Of course, you need to have <code>tensorboard --port=2024 --bind_all --logdir=. --window_title=FMR</code> running.</p> <p>Note that <code>2024</code> here is the same as the <code>Destination</code> port you chose in <code>putty</code>.</p>"},{"location":"python/","title":"Python","text":""},{"location":"python/#creating-virtual-environments","title":"Creating Virtual Environments","text":"<p>The preferred method is to use <code>uv</code> as it is much faster than other methods. uv documentation</p> <p>Install <code>uv</code></p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <p>Create your environment.</p> <pre><code>uv venv --python-preference=only-managed --python=3.12 --relocatable --prompt=CAISI venv\n</code></pre> <p>Activate your environment and start populating it with your dependencies.</p> <pre><code>source venv/bin/activate\nuv pip install XYZ\n# or\nuv pip install -r requirements.txt\n</code></pre>"},{"location":"python/#profiling-imports","title":"Profiling Imports","text":"<p>How to profile python's import statements. Used in finding expensive imports that slow down <code>--help</code>.</p> <pre><code>python -X importtime myscript.py\n</code></pre> <p>or:</p> <pre><code>PYTHONPROFILEIMPORTTIME=1 myscript.python\n</code></pre>"},{"location":"python/#simplenamespace","title":"SimpleNamespace","text":"<p>SimpleNamespace: A simple object subclass that provides attribute access to its namespace, as well as a meaningful repr.</p>"},{"location":"python/#simple-hack-for-none-jsondumps-objects","title":"Simple Hack for None json.dumps Objects","text":"<p>Specify a <code>default</code> function to process objects that are not json serializable. Note that <code>default=str</code> has to be after <code>indent</code> otherwise <code>indent</code> is not working.</p> <pre><code>print(json.dumps(my_data, indent=2, default=str))\n</code></pre>"},{"location":"python/#typer","title":"Typer","text":"<p>Disable the long exception stack.</p> <pre><code>app = typer.Typer(add_completion=False, pretty_exceptions_show_locals = False)\n</code></pre>"},{"location":"rsync/","title":"rsync","text":""},{"location":"rsync/#copy-subset-of-files","title":"Copy Subset of Files","text":"<p>rsync copy over only certain types of files using include option NOTE you MUST add <code>--include='*/'</code> to let rsync at least visit all directories.</p> <pre><code>rsync \\\n  -Parzu \\\n  -m \\\n  --include='*/' \\\n  --include='*/run_mlm.config' \\\n  --include='*/trainer.py' \\\n  --exclude='*' \\\n  xlm-roberta-large.2500.min_prob.emb \\\n  xlm-roberta-large.2500.not_sorted.emb \\\n  xlm-roberta-large.5000.min_prob.emb \\\n  xlm-roberta-large.5000.not_sorted.emb \\\n  ../mt/ \\\n  -n\n</code></pre>"},{"location":"rsync/#copy-subset-of-directories","title":"Copy Subset of Directories","text":"<p>How to use Rsync to copy only specific subdirectories (same names in several directories)</p> <pre><code>rsync \\\n  -F \\\n  -Parzu \\\n  gpsc5:/space/project/portage/models/WMT2023 \\\n  /gpfs/projects/DT/mtp/models/ \\\n  --include='*/' \\\n  --include='*/wmttest2023.splited' \\\n  --include='wmttest2023.splited/***' \\\n  --exclude='*' \\\n  -n\n</code></pre>"},{"location":"rsync/#clone-an-experiments-structure","title":"Clone an Experiment's Structure","text":"<pre><code>rsync \\\n  -Parzu \\\n  -m \\\n  --include='*/' \\\n  --include='*/expt_config' \\\n  --include='*/model_config.yaml' \\\n  --include='*/prep.sh' \\\n  --exclude='*' \\\n  ../en2fr.2024-03-19/{finetuning,recency} \\\n  .\n</code></pre>"},{"location":"slurm/","title":"SLURM","text":""},{"location":"slurm/#errors","title":"Errors","text":""},{"location":"slurm/#cuda-out-of-memory","title":"CUDA out of Memory","text":"<p><code>torch.OutOfMemoryError</code>: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 79.18 GiB of which 166.31 MiB is free. Including non-PyTorch memory, this process has 79.01 GiB memory in use. Of the allocated memory 78.50 GiB is allocated by PyTorch, and 1.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation. See documentation for Memory Management (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)</p>"},{"location":"slurm/#access-internet-from-a-node","title":"Access Internet from a Node","text":""},{"location":"slurm/#gpsc-c","title":"GPSC-C","text":"<pre><code>export https_proxy=http://webproxy.collab.science.gc.ca:8888\nexport http_proxy=http://webproxy.collab.science.gc.ca:8888\n</code></pre>"},{"location":"slurm/#gpsc","title":"GPSC","text":"<pre><code>export https_proxy=http://webproxy.science.gc.ca:8888\nexport http_proxy=http://webproxy.science.gc.ca:8888\n</code></pre>"},{"location":"slurm/#job-command-and-information","title":"Job Command and Information","text":""},{"location":"slurm/#update-a-job","title":"Update a job","text":"<p>Add a dependency to run a job after another job.</p> <pre><code>scontrol update jobid=&lt;JOBID&gt; dependency=afterok:&lt;PREVIOUS_JOBID&gt;\n</code></pre> <p>Add dependencies to multiple jobs</p> <pre><code>scontrol \\\n  update \\\n    jobid=$(echo {15882..15893} | tr ' ' ',')\n    dependency=afterok:15754:15756:15757:15758:15765:15766:15767:15740:15743\n</code></pre> <p>Change the time limit.</p> <pre><code>scontrol update jobid=&lt;JOBID&gt; TimeLimit=&lt;NEW_TIMELIMIT&gt;\n</code></pre> <p>Change the number of maximum concurrent tasks.</p> <pre><code>scontrol update jobid=&lt;JOBID&gt; ArrayTaskThrottle=0\n</code></pre>"},{"location":"slurm/#a-scriptless-job","title":"A Scriptless Job","text":"<p>Running a binary without a top level script in SLURM</p> <pre><code> sbatch \\\n   --time=00:20:00 \\\n   --partition=TrixieMain,JobTesting \\\n   --account=dt-mtp \\\n   --nodes=1 \\\n   --ntasks-per-node=1 \\\n   --cpus-per-task=40 \\\n   --mem=40G \\\n   --wrap='time python -m sockeye.translate --output-type json --batch-size 32 --models base_model --input corpora/validation.en --use-cpu &gt; model/decode.output.0.00000.json'\n</code></pre>"},{"location":"slurm/#jobs-information","title":"Job's Information","text":"<p>List detailed information for a job (useful for troubleshooting):</p> <pre><code>scontrol show jobid --details &lt;JOBID&gt;\n</code></pre> <pre><code>JobId=403641 JobName=HoC-CL.training\n   UserId=larkins(171967808) GroupId=larkins(171967808) MCS_label=N/A\n   Priority=68528 Nice=0 Account=dt-mtp QOS=normal\n   JobState=PENDING Reason=ReqNodeNotAvail,_Reserved_for_maintenance Dependency=(null)\n   Requeue=1 Restarts=1 BatchFlag=1 Reboot=0 ExitCode=0:0\n   DerivedExitCode=0:0\n   RunTime=00:00:00 TimeLimit=12:00:00 TimeMin=N/A\n   SubmitTime=2024-01-19T04:39:27 EligibleTime=2024-01-19T04:41:28\n   AccrueTime=2024-01-19T04:41:28\n   StartTime=2024-01-21T14:30:00 EndTime=2024-01-22T02:30:00 Deadline=N/A\n   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2024-01-19T10:13:15\n   Partition=TrixieMain AllocNode:Sid=hn2:20782\n   ReqNodeList=(null) ExcNodeList=cn119\n   NodeList=(null)\n   BatchHost=cn120\n   NumNodes=1 NumCPUs=24 NumTasks=4 CPUs/Task=6 ReqB:S:C:T=0:0:*:*\n   TRES=cpu=24,mem=96G,node=1,billing=24,gres/gpu=4\n   Socks/Node=* NtasksPerN:B:S:C=4:0:*:* CoreSpec=*\n   MinCPUsNode=24 MinMemoryNode=96G MinTmpDiskNode=0\n   Features=(null) DelayBoot=00:00:00\n   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\n   Command=/gpfs/projects/DT/mtp/models/HoC-ContinualLearning/nmt/tools/train.sh\n   WorkDir=/gpfs/projects/DT/mtp/models/HoC-ContinualLearning/nmt/en2fr/baseline_initial\n   Comment=House of Commons Continual Learning NMT training\n   StdErr=/gpfs/projects/DT/mtp/models/HoC-ContinualLearning/nmt/en2fr/baseline_initial/HoC-CL.training-403641.out\n   StdIn=/dev/null\n   StdOut=/gpfs/projects/DT/mtp/models/HoC-ContinualLearning/nmt/en2fr/baseline_initial/HoC-CL.training-403641.out\n   Power=\n   TresPerNode=gpu:4\n</code></pre> <p>Find out where a job will run</p> <pre><code>scontrol show job &lt;JOBID&gt;\n</code></pre>"},{"location":"slurm/#tabulate-the-information-of-queued-jobs","title":"Tabulate the Information of Queued jobs","text":"<p>If you are running a lot of jobs and need to inspect some of them for some feature, example, what command they will run, use <code>mlr</code>.</p> <pre><code>squeue --Format='JobID' --user=$USER --noheader \\\n| \\parallel 'scontrol --oneliner show job {}' \\\n| sed 's/ /,/g' \\\n| mlr --opprint cat \\\n| less\n</code></pre>"},{"location":"slurm/#stats-of-a-job","title":"Stats of a job","text":"<p>Get some stats about a job that ran on <code>Slurm</code>. Get stats about a job that has finished.</p> <pre><code>sacct --long --jobs=&lt;JOBID&gt;\nsacct -l -j &lt;JOBID&gt;\n</code></pre> <pre><code>seff 112610\n</code></pre> <pre><code>Job ID: 112610\nCluster: trixie-rhel9\nUser/Group: larkins/larkins\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 32\nCPU Utilized: 00:02:17\nCPU Efficiency: 3.79% of 01:00:16 core-walltime\nJob Wall-clock time: 00:01:53\nMemory Utilized: 2.78 GB\nMemory Efficiency: 6.96% of 40.00 GB\n</code></pre>"},{"location":"slurm/#cluster-information","title":"Cluster information","text":"<p>See what the nodes really offer.</p> <pre><code>scontrol show nodes\n</code></pre> <pre><code>NodeName=cn136 Arch=x86_64 CoresPerSocket=16\n   CPUAlloc=0 CPUTot=64 CPULoad=0.01\n   AvailableFeatures=(null)\n   ActiveFeatures=(null)\n   Gres=gpu:4\n   NodeAddr=cn136 NodeHostName=cn136\n   OS=Linux 3.10.0-1160.62.1.el7.x86_64 #1 SMP Tue Apr 5 16:57:59 UTC 2022\n   RealMemory=192777 AllocMem=0 FreeMem=176763 Sockets=2 Boards=1\n   State=IDLE ThreadsPerCore=2 TmpDisk=0 Weight=1 Owner=N/A MCS_label=N/A\n   Partitions=JobTesting\n   BootTime=2023-09-21T07:56:52 SlurmdStartTime=2023-09-21T07:57:17\n   CfgTRES=cpu=64,mem=192777M,billing=64,gres/gpu=4\n   AllocTRES=\n   CapWatts=n/a\n   CurrentWatts=0 AveWatts=0\n   ExtSensorsJoules=n/s ExtSensorsWatts=0 ExtSensorsTemp=n/s\n</code></pre> <p>See the information of an upcoming maintenance.</p> <pre><code>scontrol show reservation\n</code></pre> <pre><code>ReservationName=maintenance2 StartTime=2024-01-19T14:30:00 EndTime=2024-01-21T14:30:00 Duration=2-00:00:00\n   Nodes=cn[104-136] NodeCnt=33 CoreCnt=1056 Features=(null) PartitionName=(null) Flags=MAINT,IGNORE_JOBS,SPEC_NODES,ALL_NODES\n   TRES=cpu=2112\n   Users=root Accounts=(null) Licenses=(null) State=INACTIVE BurstBuffer=(null) Watts=n/a\n\nReservationName=maintenance3 StartTime=2024-02-03T14:30:00 EndTime=2024-02-05T14:30:00 Duration=2-00:00:00\n   Nodes=cn[104-136] NodeCnt=33 CoreCnt=1056 Features=(null) PartitionName=(null) Flags=MAINT,IGNORE_JOBS,SPEC_NODES,ALL_NODES\n   TRES=cpu=2112\n   Users=root Accounts=(null) Licenses=(null) State=INACTIVE BurstBuffer=(null) Watts=n/a\n</code></pre>"},{"location":"slurm/#available-partitions","title":"Available Partitions","text":"<pre><code>sinfo\n</code></pre> <pre><code>PARTITION          AVAIL  TIMELIMIT  NODES  STATE NODELIST\nSynthiaCPU-Preempt    up   12:00:00     14   idle cn[101-114]\nSynthiaGPU-Preempt    up   12:00:00      1  drain gn204\nSynthiaGPU-Preempt    up   12:00:00      2  alloc gn[201-202]\nSynthiaGPU-Preempt    up   12:00:00      1   idle gn203\nJobTesting            up    1:00:00      2   idle cn[115-116]\n</code></pre>"},{"location":"slurm/#my-account","title":"My Account","text":"<pre><code>sacctmgr show associations | grep ${USER}\n</code></pre> <p>Alternatively:</p> <pre><code>sacctmgr show user withassoc format=account,user,defaultaccount where user=$USER\n</code></pre> <pre><code>   Account       User   Def Acct\n---------- ---------- ----------\n   dt-base    larkins    dt-base\n</code></pre>"},{"location":"slurm/#compute-capabilities","title":"Compute Capabilities","text":"<pre><code>sbatch \\\n  --partition=JobTesting \\\n  --gres=gpu:1 \\\n  --time=00:10:00 \\\n  --cpus-per-task=1 \\\n  --tasks-per-node=1 \\\n  --nodes=1 \\\n  --mem=1G \\\n  --wrap='nvidia-smi --query-gpu=compute_cap'\n</code></pre> <pre><code>compute_cap\n7.0\n\n</code></pre>"},{"location":"slurm/#nodes-specs","title":"Node's Specs","text":"<p>Get node's specs.</p> <pre><code>sinfo --Node --responding --long\nsinfo -N -r -l\n</code></pre> NODELIST NODES PARTITION STATE CPUS S:C:T MEMORY TMP_DISK WEIGHT AVAIL_FE REASON cn101 1 TrixieMain* drained 64 2:16:2 192777 0 1 (null) update cn102 1 TrixieMain* mixed 64 2:16:2 192777 0 1 (null) none cn110 1 TrixieMain* mixed 64 2:16:2 192777 0 1 (null) none cn118 1 TrixieMain* idle 64 2:16:2 192777 0 1 (null) none cn119 1 TrixieMain* idle 64 2:16:2 192777 0 1 (null) none cn125 1 TrixieMain* idle 64 2:16:2 192777 0 1 (null) none"},{"location":"slurm/#cluster-usage","title":"Cluster Usage","text":"<p>Find the cluster usage per user.</p> <pre><code>sreport user top start=2020-06-01 end=2020-06-30 -t percent\n</code></pre> <pre><code>--------------------------------------------------------------------------------\nTop 10 Users 2020-06-01T00:00:00 - 2020-06-29T23:59:59 (2505600 secs)\nUsage reported in Percentage of Total\n--------------------------------------------------------------------------------\n  Cluster     Login     Proper Name         Account      Used   Energy\n--------- --------- --------------- --------------- --------- --------\n   trixie  stewartd         Stewart            ai4d    24.23%    0.00%\n   trixie   larkins          Larkin            ai4d    23.75%    0.00%\n   trixie   ryczkok          Ryczko             sdt     3.07%    0.00%\n   trixie      guoh             Guo    ai4d-core-06     0.16%    0.00%\n   trixie       loc              Lo            ai4d     0.13%    0.00%\n   trixie   valdesj          Valdes              dt     0.00%    0.00%\n   trixie       xip              Xi              dt     0.00%    0.00%\n   trixie     asadh                        covid-02     0.00%    0.00%\n   trixie     paulp            Paul            ai4d     0.00%    0.00%\n   trixie    ebadia           Ebadi              dt     0.00%    0.00%\n</code></pre>"},{"location":"slurm/#allocated-hostnames-of-a-multinode-job","title":"Allocated Hostnames of a Multinode Job","text":"<p>Get a list of hostnames allocated to a multi node job.</p> <pre><code>scontrol show hostnames $SLURM_NODELIST\n</code></pre> <pre><code>cn101\ncn102\ncn103\ncn104\n</code></pre>"},{"location":"slurm/#gpsc5","title":"GPSC5","text":""},{"location":"slurm/#ssh-to-a-worker-node","title":"SSH to a Worker Node","text":"<p>This is an example command to connect to a worker node on GPSC5</p> <pre><code>srun --overlap --pty --jobid=&lt;JOBID&gt; bash -l\n</code></pre> <p>Example of connecting to a GPU running job on GPSC5.</p> <pre><code>srun --overlap --gres=gpu:1 --nodes=1 --ntasks=1 --mem-per-cpu=0 --pty --oversubscribe --jobid=&lt;SLURM_JOBID&gt; /bin/bash -l\n</code></pre> <pre><code>srun \\\n  --overlap \\\n  --oversubscribe \\\n  --pty \\\n  --gres=gpu:0 \\\n  --nodes=1 \\\n  --ntasks=1 \\\n  --mem-per-cpu=0 \\\n  --jobid=&lt;JOBID&gt; \\\n  /bin/bash -l\n</code></pre>"},{"location":"slurm/#gpscc","title":"GPSCC","text":""},{"location":"slurm/#script-example","title":"Script Example","text":"<p>The following example uses multiple nodes.</p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=train\n#SBATCH --partition=gpu_a100\n#SBATCH --account=nrc_ict__gpu_a100\n\n#SBATCH --time=2880\n\n##IMPORTANT: `#SBATCH --ntasks-per-node=2`  MUST match  `#SBATCH --gres=gpu:2`\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=2\n#SBATCH --gres=gpu:2\n\n#SBATCH --output=%x-%j.out\n#SBATCH --error=%x-%j.err\n#SBATCH --open-mode=append\n#SBATCH --mail-user==tes001\n#SBATCH --mail-type=NONE\n\n#SBATCH --signal=B:15@30\n\n\nulimit -v unlimited\n\ncd /home/tes001/DT/tes001/\nsource ./SETUP_PT2.source\ncd /home/tes001/DT/tes001/LJSpeech-1.1/PT2\n\nsrun everyvoice train text-to-spec --devices 2 --nodes 1 config/everyvoice-text-to-spec.yaml\n</code></pre>"},{"location":"slurm/#sleeper-job","title":"Sleeper Job","text":"<p>Start a sleeper job</p> <pre><code>psub -N sleeper -Q nrc_ict -gpu 'sleep 3600'\n</code></pre>"},{"location":"slurm/#connect-to-node","title":"Connect to Node","text":"<pre><code>srun \\\n  --overlap \\\n  --oversubscribe \\\n  --pty \\\n  --nodes=1 \\\n  --ntasks=1 \\\n  --mem-per-cpu=0 \\\n  --jobid=&lt;JOBID&gt; \\\n  /bin/bash -l\n</code></pre>"},{"location":"slurm/#running-pytorch-on-multiple-nodes","title":"Running pytorch on Multiple Nodes","text":""},{"location":"slurm/#acceleratehuggingface-specific","title":"Accelerate/HuggingFace Specific","text":"<pre><code>#!/bin/bash\n\n# [SLURM/ACCELERATE](https://github.com/huggingface/accelerate/blob/main/examples/slurm/submit_multinode.sh)\n# [MNIST](https://huggingface.co/blog/pytorch-ddp-accelerate-transformers)\n\n#SBATCH --partition=TrixieMain\n#SBATCH --account=dt-mtp\n\n#SBATCH --job-name=MNIST.distributed\n\n#SBATCH --time=12:00:00\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=8\n#SBATCH --gres=gpu:4\n\n#SBATCH --output=%x-%j.out\n#SBATCH --signal=B:USR1@30\n#SBATCH --requeue\n\n# Requeueing on Trixie\n# [source](https://www.sherlock.stanford.edu/docs/user-guide/running-jobs/)\n# [source](https://hpc-uit.readthedocs.io/en/latest/jobs/examples.html#how-to-recover-files-before-a-job-times-out)\n\nfunction _requeue {\n   echo \"BASH - trapping signal 10 - requeueing $SLURM_JOBID\"\n   date\n   # This would allow to generically requeue any job but since we are using XLM\n   # which is slurm aware, XLM could save its model before requeueing.\n   scontrol requeue \"$SLURM_JOBID\"\n}\n\nif [[ -n \"$SLURM_JOBID\" ]]; then\n   trap _requeue USR1\nfi\n\n\n# Keep a copy of the code in case it changes between runs.\nhead -n 112312 \"$0\" my_huggingface_trainer.py\n\n# Setup your working environment.\nsource setup_env.sh \"\"\n\n# These are required to setup the distributed framework.\nexport TQDM_MININTERVAL=90\nhead_node_ip=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1)\nreadonly head_node_ip\nreadonly head_node_port=$(( SLURM_JOBID % (50000 - 30000 + 1 ) + 30000 ))\nexport launcher=\"accelerate launch \\\n  --mixed_precision=fp16 \\\n  --num_processes=$((SLURM_NNODES * SLURM_GPUS_ON_NODE)) \\\n  --num_machines=$SLURM_NNODES \\\n  --num_cpu_threads_per_process=$SLURM_CPUS_PER_TASK \\\n  --rdzv_backend=c10d \\\n  --main_process_ip=$head_node_ip \\\n  --main_process_port=$head_node_port \\\n  --machine_rank=$SLURM_NODEID \\\n  \"\n\n# Dump the environment.\n( set -o posix ; set )\n\n\nfunction task {\n# NOTE: You may have to provide options to your script to detect nodes/GPUs `--trainer.num_nodes=\"$SLURM_NNODES\"`.\n  srun $launcher my_huggingface_trainer.py\n# or\n  srun $launcher -m my_python_module\n}\n\n# WARNING: You must sent your in the background in order for the requeueing mechanism to work.\ntask &amp;\nwait\n</code></pre> <p>Question: using pytorch lightining using multiple nodes and multiple gpu per node</p> <p>To train a PyTorch Lightning model across multiple nodes with multiple GPUs per node, configure the <code>Trainer</code> with the appropriate parameters. Use the <code>accelerator=\"gpu\"</code>, devices to specify the number of GPUs per node, <code>num_nodes</code> to define the total number of nodes, and <code>strategy=\"ddp\"</code> for distributed data parallelism. For example, to train on 32 GPUs across 4 nodes with 8 GPUs per node, the configuration would be:</p> <pre><code>trainer = Trainer(\n    accelerator=\"gpu\",\n    devices=8,\n    num_nodes=4,\n    strategy=\"ddp\",\n    max_epochs=10\n)\n</code></pre> <p>This setup leverages PyTorch Lightning's built-in DDP support to manage distributed training across nodes. The devices parameter can also be set to \"auto\" to automatically detect available GPUs.</p> <p>When using a job scheduler like SLURM, ensure the job script requests the correct resources. For a 4-node setup with 8 GPUs per node, the SLURM header should include:</p> <pre><code>#SBATCH --nodes=4\n#SBATCH --gres=gpu:8\n#SBATCH --ntasks-per-node=8\n</code></pre> <p>The <code>srun</code> command should launch the training script, and PyTorch Lightning will automatically use environment variables like <code>SLURM_NTASKS</code> and <code>SLURM_NODEID</code> to coordinate the training process across nodes. It is also recommended to disable file locks in the environment to prevent initialization issues when the same environment is accessed from multiple nodes simultaneously.</p> <p>For cluster-specific configurations, such as on Jean-Zay, use <code>--ntasks-per-node=8</code> to assign one task per GPU and ensure exclusive node reservation for full memory access. Additionally, set NCCL-related environment variables to avoid communication conflicts between GPUs, such as export <code>NCCL_DEBUG=INFO</code> for debugging connectivity issues.</p>"},{"location":"slurm/#submitting-on-multiple-cluster","title":"Submitting on Multiple Cluster","text":"<p>On GPSC, we can now submit up to 1000 6 hour-job with <code>#SBATCH --qos=low</code>.</p> <pre><code>sbatch \\\n  --time=00:10:00 \\\n  --clusters=gpsc5,gpsc6,gpsc7,gpsc8 \\\n  --account=${NRC_RC} \\\n  --partition=standard \\\n  myjob.sh\n</code></pre>"},{"location":"slurm/#templateslurm","title":"Template.slurm","text":"<pre><code>#!/bin/bash\n# vim:syntax=bash:\n\n# sbatch WMT2025_eval.slurm\n\n#SBATCH --job-name=WMT25.eval\n#SBATCH --comment=\"Official WMT2025 LRSL eval script\"\n\n# Trixie\n#SBATCH --partition=TrixieMain,JobTesting\n#SBATCH --account=dt-mtp\n# On GPSC7\n##SBATCH --partition=gpu_a100\n##SBATCH --account=nrc_ict__gpu_a100\n# On GPSC5\n##SBATCH --partition=gpu_v100\n##SBATCH --account=nrc_ict__gpu_v100\n# On GPSC-C\n##SBATCH --partition=gpu_a100\n##SBATCH --account=nrc_ict__gpu_a100\n##SBATCH --comment=\"image=nrc/nrc_all_default_ubuntu-22.04-amd64_latest\"\n##SBATCH --comment=\"image=registry.maze-c.collab.science.gc.ca/sschpcs/generic-job:ubuntu22.04_master\"\n\n#SBATCH --gres=gpu:1\n#SBATCH --time=12:00:00\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=32\n#SBATCH --mem=6G\n\n#SBATCH --open-mode=append\n#SBATCH --output=%x-%j.out\n\n#SBATCH --requeue\n#SBATCH --signal=B:USR1@30\n\n# Fix SLURM environment variables.\nSLURM_JOB_CPUS_PER_NODE=${SLURM_JOB_CPUS_PER_NODE%%(*)}   # '24(x2)' =&gt; '24'\nSLURM_JOB_CPUS_PER_NODE_PACK_GROUP_0=${SLURM_JOB_CPUS_PER_NODE_PACK_GROUP_0%%(*)}   # '24(x2)' =&gt; '24'\nSLURM_STEP_TASKS_PER_NODE=${SLURM_STEP_TASKS_PER_NODE%%(*)}   # '4(x2)' =&gt; '4'\nSLURM_TASKS_PER_NODE=${SLURM_TASKS_PER_NODE%%(*)}   # '4(x2)' =&gt; '4'\n\n# NOTE: We set OMP_NUM_THREADS or else we get the following Warning:\n# WARNING:torch.distributed.run:\n# *****************************************\n# Setting OMP_NUM_THREADS environment variable for each process to be 1 in\n# default, to avoid your system being overloaded, please further tune the\n# variable for optimal performance in your application as needed.\n# *****************************************\nexport OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK:-$(nproc)}\n\n# If reserved but unallocated memory is large try setting\n# PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.\n# See documentation for Memory Management\n# (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n#PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n\n# Requeueing on Trixie\n# [source](https://www.sherlock.stanford.edu/docs/user-guide/running-jobs/)\n# [source](https://hpc-uit.readthedocs.io/en/latest/jobs/examples.html#how-to-recover-files-before-a-job-times-out)\nfunction _requeue {\n   echo \"BASH - trapping signal 10 - requeueing $SLURM_JOBID\"\n   date\n   # This would allow to generically requeue any job but since we are using XLM\n   # which is slurm aware, XLM could save its model before requeueing.\n   scontrol requeue \"$SLURM_JOBID\"\n}\n\nif [[ -n \"$SLURM_JOBID\" ]]; then\n  declare -a FORMAT_STRING=(\n    JobID\n    Submit\n    Start\n    End\n    Elapsed\n    ExitCode\n    State\n    CPUTime\n    MaxRSS\n    MaxVMSize\n    MaxDiskRead\n    MaxDiskWrite\n    AllocCPUs\n    AllocTRES%-50\n    NodeList\n    JobName%-30\n    Comment%-80\n  )\n  SACCT_FORMAT=$(IFS=\",\"; echo \"${FORMAT_STRING[*]}\")\n  export SACCT_FORMAT\n  trap \"sacct --jobs $SLURM_JOBID --format=$SACCT_FORMAT\" 0\n  trap _requeue USR1\n  unset FORMAT_STRING\nfi\n\nhead -n 123123 \"$0\" &gt;&amp;2\n\n\n# Output debugging information\nfunction debug_info {\n  echo \"DEBUGGING INFO\" &gt;&amp;2\n\n  {\n    date\n    hostname\n    uname --all\n    cat /etc/issue\n    pwd\n    command -v python\n    # [How to list variables declared in script in bash?](https://stackoverflow.com/a/1305273)\n    # In section SHELL BUILTIN COMMANDS (in the set section) it says: \"In\n    # posix mode, only shell variables are listed.\"\n    ( set -o posix; set; )\n    tr \":\" \"\\n\" &lt;&lt;&lt; \"$LD_LIBRARY_PATH\"\n    # What conda/uv environment are we in?\n    # Record all package versions.\n    if [[ -n \"$VIRTUAL_ENV\" ]]; then\n      echo \"VENV: $VIRTUAL_ENV\"\n      command -v uv\n      uv pip list\n      uv pip freeze\n    fi\n    if [[ -n \"$CONDA_DEFAULT_ENV\" ]]; then\n      command -v conda\n      conda env export\n    fi\n  } &gt;&amp;2\n\n  {\n    # What GPU id is assigned to this job.\n    nvidia-smi --list-gpus\n    nvidia-smi\n  } | sed 's/^/   /' &gt;&amp;2\n\n  echo \"DEBUGGING INFO END\"\n  echo;echo;echo\n}\n\n# GPSC\n# [[ $JOBCTL_SLURM_CELLS =~ gpsc[^3] ]] &amp;&amp; export https_proxy=http://webproxy.science.gc.ca:8888\n# [[ $JOBCTL_SLURM_CELLS =~ gpsc[^3] ]] &amp;&amp; export http_proxy=http://webproxy.science.gc.ca:8888\n# GPSC-C\n# [[ $JOBCTL_SLURM_CELLS =~ gpscc3 ]] &amp;&amp; export https_proxy=http://webproxy.collab.science.gc.ca:8888\n# [[ $JOBCTL_SLURM_CELLS =~ gpscc3 ]] &amp;&amp; export http_proxy=http://webproxy.collab.science.gc.ca:8888\nexport TQDM_MININTERVAL=90\nhead_node_ip=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1)\nreadonly head_node_ip\nreadonly head_node_port=$(( SLURM_JOBID % (50000 - 30000 + 1 ) + 30000 ))\n\n\nsource venv/bin/activate \"\"\n\n# Call this after you have setup your environemnt and all your local variables\ndebug_info\n\nfunction task {\n  command time --portability lm_eval \\\n    --model hf \\\n    --model_args pretrained=unsloth/Qwen2.5-3B-Instruct-unsloth-bnb-4bit \\\n    --tasks sorbian \\\n    --device cuda:0 \\\n    --batch_size 8 \\\n    --output_path baseline_output_sorbian \\\n    --log_samples\n}\n\n# WARNING: You must sent your in the background in order for the requeueing mechanism to work.\ntask &amp;\nwait\n</code></pre>"},{"location":"tmux/","title":"<code>tmux</code>","text":""},{"location":"tmux/#exporting-a-variable","title":"Exporting a Variable","text":"<p>tmux is exporting an environment variable that is no longer being exported in .bashrc</p> <pre><code>tmux set-environment -gru FZF_TMUX_OPTS\n</code></pre> <p>Fix the dark colors in <code>tmux</code> when <code>$TERM</code> is not correctly set.</p> <pre><code>tmux set-environment -g TERM \"putty-256color\"\n</code></pre> <pre><code>set-environment [-Fhgru] [-t target-session] name [value]\n    (alias: setenv)\n  Set or unset an environment variable.\n  If -g is used, the change is made in the global environment; otherwise, it is applied to the session environment for target-session.\n  If -F is present, then value is expanded as a format.\n  The -u flag unsets a variable.\n  -r indicates the variable is to be removed from the environment before starting a new process.\n  -h marks the variable as hidden.\n</code></pre>"},{"location":"tmux/#find-a-panes-running-a-command","title":"Find a Panes Running a Command","text":"<pre><code>find-window [-iCNrTZ] [-t target-pane] match-string\n    (alias: findw)\n  Search for a fnmatch(3) pattern or, with -r, regular expression match-string in window names, titles, and visible content (but not history).\n  The flags control matching behavior: -C matches only visible window contents, -N matches only the window name and -T matches only the window title.\n  -i makes the search ignore case.\n  The default is -CNT.\n  -Z zooms the pane.\n</code></pre> <p>This command works only if at least one client is attached.</p> <pre><code>CTRL+b + f\n&lt;SEARCH STRING&gt;\n</code></pre>"},{"location":"tmux/#rename-a-pane","title":"Rename a pane","text":"<pre><code>set -g pane-border-status top\nset -g pane-border-format \" [ ###P #T ] \"\n</code></pre> <pre><code>CTRL+b + :\nselect-pane -T \"title\"\n</code></pre>"},{"location":"tmux/#sharing-a-window-across-sessions","title":"Sharing a Window Across Sessions","text":"<p>To share a window between two sessions: <code>tmux link-window -s &lt;src-window&gt; -t &lt;dst-window&gt;</code></p>"},{"location":"vscode/","title":"VSCode","text":""},{"location":"vscode/#setup","title":"Setup","text":""},{"location":"vscode/#plugins","title":"Plugins","text":"<p>Install <code>Vim</code> plugin to feel more at home. Install <code>Remote Explorer</code> Install <code>Remote Development</code> plugin which include 4 plugins:</p> <ul> <li><code>WSL</code></li> <li><code>Dev Containers</code></li> <li><code>Remote - SSH</code></li> <li><code>Remote - Tunnels</code></li> </ul>"},{"location":"vscode/#remote","title":"Remote","text":"<p>Create a remote connection by clicking the <code>Remote Explorer</code> plugin icon in the left bar. Click the <code>+</code> at the end of the <code>SSH</code>. A popup will appear. Follow the instructions to add a new remote host. You may have to perform some sort of refresh to see your new connection under <code>SSH</code>. Click your new connection and follow the instructions to finish setting up your remote connection.</p>"}]}